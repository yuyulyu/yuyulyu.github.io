---
title: "ğŸ‰ğŸƒ ç¬¬ 3 ç« : çº¿æ€§æ¨¡å‹"
description: 
author: yoyo
date: 2024-09-21 12:00:00 +0800
categories: [Artificial Intelligence, Datawhale, Pumpkin]
tags: [Machine Learning Theory]
math: true
lang: zh-CN
---

## çº¿æ€§æ¨¡å‹

ç»™å®šä¸€ä¸ªåŒ…å« \( d \) ä¸ªå±æ€§çš„ç¤ºä¾‹ \( x = (x_1, x_2, ..., x_d) \)ï¼Œæˆ‘ä»¬è¯•å›¾å­¦ä¹ ä¸€ä¸ªæƒé‡å‘é‡ \( \omega = (\omega_1, \omega_2, ..., \omega_d) \) å’Œåç½®é¡¹ \( b \)ï¼Œä»è€Œç¡®å®šç”¨äºé¢„æµ‹çš„çº¿æ€§å‡½æ•°ã€‚

çº¿æ€§æ¨¡å‹çš„é¢„æµ‹å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
f(x) = \omega_1 x_1 + \omega_2 x_2 + \dots + \omega_d x_d + b
$$

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¯ä»¥ç”¨å‘é‡çš„å½¢å¼ç®€æ´åœ°è¡¨ç¤ºä¸ºï¼š

$$
f(x) = \omega^T x + b
$$

å…¶ä¸­ï¼Œ\( \omega^T x \) è¡¨ç¤ºæƒé‡å‘é‡ \( \omega \) å’Œå±æ€§å‘é‡ \( x \) çš„å†…ç§¯ï¼Œ\( b \) ä¸ºåç½®é¡¹ã€‚

çº¿æ€§æ¨¡å‹å½¢å¼ç®€å•ï¼Œæ˜“äºå»ºæ¨¡ï¼Œå…·æœ‰è¾ƒå¼ºçš„å¯è§£é‡Šæ€§ï¼ˆcomprehensibility and understandabilityï¼‰ã€‚æƒé‡ \( \omega \) ç›´è§‚åœ°è¡¨ç¤ºäº†å„å±æ€§åœ¨é¢„æµ‹ä¸­çš„é‡è¦æ€§ï¼Œå› æ­¤ä¹Ÿè¢«ç§°ä¸ºæƒé‡ï¼ˆweightï¼‰ã€‚æ¨¡å‹çš„æ¯ä¸ªæƒé‡å¯ä»¥è§£é‡Šè¯¥å±æ€§å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®ã€‚

## çº¿æ€§å›å½’ Linear regression

ç»™å®šæ•°æ®é›† \( D = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\} \)ï¼Œå³ \( D = \{(x_i, y_i)\}_{i=1}^m \)ï¼Œå…¶ä¸­ \( x_i \) ä¸ºè¾“å…¥ï¼Œ\( y_i \) ä¸ºå¯¹åº”çš„è¾“å‡ºå€¼ã€‚çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯é€šè¿‡æ•°æ®é›†å­¦ä¹ ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œç”¨æ¥é¢„æµ‹æ–°çš„æ•°æ®ç‚¹ã€‚

- å¯¹å­˜åœ¨â€œåºâ€(order)å…³ç³»çš„ç¦»æ•£å±æ€§ï¼š
  - é€šè¿‡è¿ç»­åŒ–æ±Ÿå…¶è½¬åŒ–ä¸ºè¿ç»­å€¼
  - ä¾‹å¦‚ï¼Œå±æ€§â€œé«˜åº¦â€å…·æœ‰â€œé«˜â€ã€â€œä¸­â€ã€â€œä½â€ä¸‰ä¸ªç¦»æ•£å–å€¼ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨åºå…³ç³»ï¼Œå¯ä»¥å°†å…¶è½¬åŒ–ä¸ºè¿ç»­å€¼ï¼Œå¦‚ {1.0, 0.5, 0.0}ã€‚
- å¯¹ä¸å­˜åœ¨â€œåºâ€å…³ç³»çš„ç¦»æ•£å±æ€§ï¼š
  - è‹¥å¯¹æ— åºå±æ€§è¿›è¡Œè¿ç»­åŒ–ï¼Œä¼šä¸æ°å½“åœ°å¼•å…¥åºå…³ç³»ï¼Œå¯èƒ½å¯¼è‡´è¯¯å¯¼æ€§çš„å¤„ç†ï¼ˆå¦‚åœ¨è·ç¦»è®¡ç®—ä¸­ï¼‰ã€‚é€šå¸¸ï¼Œå°† \( k \) ä¸ªæ— åºå±æ€§å€¼è½¬åŒ–ä¸º \( k \) ç»´å‘é‡ï¼ˆä¹Ÿç§°ä¸ºâ€œç‹¬çƒ­ç¼–ç â€æˆ–â€œone-hot encodingâ€ï¼‰ã€‚
  - ä¾‹å¦‚ï¼Œå±æ€§â€œç±»åˆ«â€å–å€¼ä¸ºâ€œAâ€ã€â€œBâ€ã€â€œCâ€ï¼Œå¯ä»¥å°†å…¶ç¼–ç ä¸º (0, 0, 1)ã€(0, 1, 0)ã€(1, 0, 0)ï¼Œè€Œä¸ä¼šå¼•å…¥é”™è¯¯çš„é¡ºåºå…³ç³»ã€‚
 
**åº Order**ï¼šåºå…³ç³»æŒ‡å±æ€§å€¼ä¹‹é—´å­˜åœ¨å…ˆåæˆ–å¤§å°å…³ç³»ã€‚ä¾‹å¦‚ï¼Œâ€œé«˜â€ã€â€œä¸­â€ã€â€œä½â€æœ‰æ˜æ˜¾çš„åºå…³ç³»ï¼Œè€Œâ€œçº¢â€ã€â€œç»¿â€ã€â€œè“â€åˆ™æ²¡æœ‰åºå…³ç³»ã€‚

çº¿æ€§å›å½’è¯•å›¾å­¦ä¹ å¦‚ä¸‹å½¢å¼çš„çº¿æ€§æ¨¡å‹ï¼š

$$
f(x_i) = \omega^T x_i + b
$$

å…¶ä¸­ï¼Œ\( \omega \) ä¸ºæƒé‡å‘é‡ï¼Œ\( x_i \) ä¸ºè¾“å…¥å±æ€§å‘é‡ï¼Œ\( b \) ä¸ºåç½®é¡¹ã€‚çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹ \( f(x_i) \) å¯¹åº”çš„é¢„æµ‹å€¼å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼ \( y_i \)ï¼Œå³ï¼š

$$
f(x_i) \approx y_i
$$

### æŸå¤±å‡½æ•° cost function

#### å‡åˆ†æ–¹å·® Mean square error (MSE)

**å‡åˆ†æ–¹å·®**ä¹Ÿç§°ä¸º**å¹³æ–¹æŸå¤±å‡½æ•°** (Square Loss)ã€‚è¿™ä¸€æŸå¤±å‡½æ•°å¯¹åº”äº†å¸¸ç”¨çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆç®€ç§°â€œæ¬§æ°è·ç¦»â€ Euclidean distanceï¼‰ã€‚æŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š

$$
(\omega^*, b^*) = \underset{\omega, b}{\arg\min} \sum_{i=1}^{m} (f(x_i) - y_i)^2
$$

MSE å¯¹å¤§è¯¯å·®æƒ©ç½šè¾ƒå¤§ï¼Œé€‚ç”¨äºè¯¯å·®æœä»æ­£æ€åˆ†å¸ƒçš„åœºæ™¯ã€‚

#### å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute Error (MAE)

è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„ç»å¯¹å·®å€¼ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$
\text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |f(x_i) - y_i|
$$

MAE å¯¹ç¦»ç¾¤ç‚¹æ•æ„Ÿåº¦è¾ƒä½ï¼Œé€‚ç”¨äºå«æœ‰å™ªå£°æˆ–ç¦»ç¾¤ç‚¹çš„æ•°æ®é›†ã€‚

#### Huber æŸå¤± Huber Loss

ç»“åˆäº† MSE å’Œ MAE çš„ä¼˜ç‚¹ï¼Œé€‚ç”¨äºæ—¢åŒ…å«å°è¯¯å·®åˆåŒ…å«ç¦»ç¾¤ç‚¹çš„åœºæ™¯ï¼š

$$
L_{\delta}(a) =
\begin{cases} 
    \frac{1}{2}a^2 & \text{if } |a| \leq \delta \\
    \delta(|a| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

å…¶ä¸­ \( a = f(x_i) - y_i \)ï¼Œ \( \delta \) æ˜¯è°ƒèŠ‚å‚æ•°ã€‚

#### å¯¹æ•°æŸå¤± Logarithmic Loss (Log Loss)

ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯äºŒåˆ†ç±»é—®é¢˜ã€‚è®¡ç®—æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼š

$$
\text{Log Loss} = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(f(x_i)) + (1 - y_i) \log(1 - f(x_i)) \right]
$$

#### äº¤å‰ç†µæŸå¤± Cross Entropy Loss

ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š

$$
\text{Cross Entropy} = -\sum_{i=1}^{m} \sum_{c=1}^{C} y_{ic} \log(f_{ic})
$$

å…¶ä¸­ \( C \) æ˜¯ç±»åˆ«æ•°ï¼Œ\( y_{ic} \) æ˜¯æ ·æœ¬ \( i \) çš„çœŸå®ç±»åˆ«ï¼Œ\( f_{ic} \) æ˜¯é¢„æµ‹æ¦‚ç‡ã€‚


### æœ€å°äºŒä¹˜æ³• least square method

æœ€å°äºŒä¹˜æ³•æ˜¯ä¸€ç§ä¼˜åŒ–ç­–ç•¥ï¼Œç›®çš„æ˜¯æœ€å°åŒ–**å‡æ–¹è¯¯å·®**ã€‚é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œæœ€å°äºŒä¹˜æ³•èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜çš„å‚æ•° \( \omega \) å’Œ \( b \)ï¼Œä»è€Œä½¿å¾—æ¨¡å‹çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚æœ€å°ã€‚è¿™ä¸€ä¼˜åŒ–è¿‡ç¨‹é€šå¸¸è¢«ç§°ä¸ºçº¿æ€§å›å½’æ¨¡å‹çš„**æœ€å°äºŒä¹˜å‚æ•°ä¼°è®¡** (parameter estimation)ã€‚

### å¤šå…ƒçº¿æ€§å›å½’ Multiple linear regression

å½“æ ·æœ¬ç”±d > 1ä¸ªå±æ€§æè¿°æ—¶

$$
f(x_i) = \omega ^ T x_i + b

æ‰€ä»¥ omegaã€omegaä¸Šé¢æœ‰ä¸€ä¸ª^ã€‘ = argmin ã€ä¸‹é¢æœ‰ä¸€ä¸ªomegaå¸¦ç€^ã€‘(y - X_omegaå¸¦ä¸ª^ï¼‰^T (y - X_omegaå¸¦ä¸ª^)

### å¯¹æ•°çº¿æ€§å›å½’ log-linear regression

è®©e^(omega^T + b) é€¼è¿‘yã€‚å½¢å¼ä¸Šä»æ˜¯çº¿æ€§å›å½’ï¼Œå®è´¨ä¸Šæ˜¯åœ¨æ±‚å»è¾“å…¥ç©ºé—´åˆ°è¾“å‡ºç©ºé—´çš„éçº¿æ€§å‡½æ•°æ˜ å°„ã€‚

[image]:

